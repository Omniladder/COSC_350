{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebc1c23b-1baf-407e-9b31-6b23cdcb0a3f",
   "metadata": {},
   "source": [
    "In This homework You will take a data set and build a model from the data to predict the target and evaluate the preformance using cross validation and grid search. If you are building a classification model you should adress which measure of accuracy is important and incorperate that in your analysis. There are four datasets that have been provided. Information for each of them can be found here:\n",
    "\n",
    "Climate failure (pop_failures_fixed.dat):\n",
    "\n",
    "https://archive.ics.uci.edu/dataset/252/climate+model+simulation+crashes\n",
    "\n",
    "Forest Fires (forestfires.csv)\n",
    "\n",
    "https://archive.ics.uci.edu/dataset/162/forest+fires\n",
    "\n",
    "Wine Quality (winequality-white.csv,whinequality-red.csv)\n",
    "\n",
    "https://archive.ics.uci.edu/dataset/186/wine+quality\n",
    "\n",
    "If you wnat to choose your own dataset you can use the following link:\n",
    "\n",
    "https://archive.ics.uci.edu/datasets\n",
    "\n",
    "You should first state the question you are trying to answer whith the model and then read in the data and make sure that the data is cleaned and ready for modeling. \n",
    "\n",
    "Next you may want to do some exploratory data analysis and convert and or scale any features you may want to use. You do not need to use all the fetures in the dataset, but you should have an explaination for any features that you drop.\n",
    "\n",
    "Next build an appropriate model to answer your question. Determine how well your model can answer your question  using cross-validation and grid search.\n",
    "\n",
    "Finally communicate your findings in a few paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b27a5f68-a231-4cc9-8040-fbc17b0911d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run import.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eebef6-5e4a-4ded-ae12-6020578723ad",
   "metadata": {},
   "source": [
    "Can you tell which Month it is based on current severity and conditions of a forest fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7dbde6ca-59d2-400a-9a29-c7a82d90f90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "#Grabs x and y valeus from df_fires\n",
    "\n",
    "df_fires=pd.read_csv('forestfires.csv')\n",
    "\n",
    "x = df_fires\n",
    "y = df_fires.loc[:, 'month']\n",
    "\n",
    "\n",
    "\n",
    "x.drop(columns=['month', 'day'], inplace=True) \n",
    "#Remove month because its y value and day because it doesnt relate with the forest fire conditions or environment\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#This Section of Code is for Scaling\n",
    "\n",
    "#Creates Standard Scaler Object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "#Standard Scaler on X Values\n",
    "columns = x.columns #Keeps Columns for when they get lost in transformations\n",
    "x = pd.DataFrame(scaler.fit_transform(x), columns = columns)\n",
    "\n",
    "#Testing if all data is normal so I could normalize data that wasn't\n",
    "for column in x.columns:\n",
    "    shapiro_stat, shapiro_p_value = stats.shapiro(x.loc[:, column])\n",
    "    if shapiro_p_value > .05:\n",
    "        print(f\"{column} is not normal\")\n",
    "\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size=0.20, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b7f5f2a9-c7ca-43bc-a59f-23ed237bb8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omniladder/.local/lib/python3.12/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Estimator: SVC(decision_function_shape='ovo', kernel='linear')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "\n",
    "# Define the SVM model\n",
    "model = svm.SVC()\n",
    "\n",
    "# Parameter grid without 'precomputed'\n",
    "parameters = {\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'gamma' : ['scale', 'auto'],\n",
    "    'shrinking' : [True, False],\n",
    "    'decision_function_shape' : ['ovo', 'ovr'],\n",
    "    'class_weight' : [None, 'balanced'],\n",
    "}\n",
    "\n",
    "# Use GridSearchCV with cross-validation\n",
    "gridModel = GridSearchCV(model, param_grid=parameters, cv=5)\n",
    "\n",
    "# Fit the model\n",
    "gridModel.fit(xTrain, yTrain)\n",
    "\n",
    "# Print test set score\n",
    "print(\"Best Estimator: {}\".format(gridModel.best_estimator_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6eeb9ab0-6d6c-4112-8c26-16a3a0d7dfc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set score: 0.85\n",
      "Test set score: 0.29\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         apr       0.00      0.00      0.00         2\n",
      "         aug       1.00      0.80      0.89        44\n",
      "         dec       1.00      1.00      1.00         3\n",
      "         feb       0.25      0.50      0.33         2\n",
      "         jan       0.00      0.00      0.00         1\n",
      "         jul       0.75      1.00      0.86         3\n",
      "         jun       1.00      1.00      1.00         4\n",
      "         mar       0.86      1.00      0.92        12\n",
      "         may       0.00      0.00      0.00         1\n",
      "         oct       0.67      1.00      0.80         2\n",
      "         sep       0.78      0.93      0.85        30\n",
      "\n",
      "    accuracy                           0.85       104\n",
      "   macro avg       0.57      0.66      0.60       104\n",
      "weighted avg       0.85      0.85      0.84       104\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         apr       0.00      0.00      0.00         2\n",
      "         aug       0.00      0.00      0.00        44\n",
      "         dec       0.00      0.00      0.00         3\n",
      "         feb       0.00      0.00      0.00         2\n",
      "         jan       0.00      0.00      0.00         1\n",
      "         jul       0.00      0.00      0.00         3\n",
      "         jun       0.00      0.00      0.00         4\n",
      "         mar       0.00      0.00      0.00        12\n",
      "         may       0.00      0.00      0.00         1\n",
      "         oct       0.00      0.00      0.00         2\n",
      "         sep       0.29      1.00      0.45        30\n",
      "\n",
      "    accuracy                           0.29       104\n",
      "   macro avg       0.03      0.09      0.04       104\n",
      "weighted avg       0.08      0.29      0.13       104\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omniladder/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/omniladder/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/omniladder/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/omniladder/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/omniladder/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/omniladder/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "\n",
    "badModel = DummyClassifier()\n",
    "badModel.fit(xTrain, yTrain)\n",
    "\n",
    "# Print test set score\n",
    "print(\"Test set score: {:.2f}\".format(gridModel.score(xTest, yTest)))\n",
    "print(\"Test set score: {:.2f}\".format(badModel.score(xTest, yTest)))\n",
    "\n",
    "yPred = gridModel.predict(xTest)\n",
    "print(classification_report(yTest, yPred))\n",
    "\n",
    "yPred = badModel.predict(xTest)\n",
    "print(classification_report(yTest, yPred))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "28635268-10e8-4d1f-9d6b-60423bfe4659",
   "metadata": {},
   "source": [
    "Using the previous results we see that regression algorithms to identify month based on a forest fire is very possible. The model above shows strong f1 scores across the 3 months of major support March August and September. However for our question recall should be most important as we are assuming that if you were a person in that month that you would be able to predict the month based on conditions and environment of forest fire meaning we only really care if a different forest fire is classified as this month. This is especially good for the months of September and March Recall is signifcantly better than accuracy however weaker on the month of august.\n",
    "\n",
    "These results are excessivly far above the Dummy Models accuracy. It is beyond reasonable to assume that the given model is in fact able to predict the month based on forest fire conditions in combination with its environment. It is worth noting that due to low data in multiple months there is an underlying assumption that because the highly populated months have had strong coorelations with that differentiate them from one another that with more data on some of the sparse data months similar coorelations will emerge that can be deciphered. I am not able to test the hypothesis to the full extent that I would like due to the lack of data in these months.\n",
    "\n",
    "In regards to process of obtaining these results as usual I started by going through data. I had removed days from the dataset as it was as far as I could tell the only column that was unrelated to my questions since day of the week is not part of the conditions of nor is it part of any environmental effects that could be helpful that I was concerned with. I then took all the data and used a standard scaler on it in order to potentially improve accuracy of the algorithm as this has benefitted me in the past. I also used past experience from HW2 and attempted to normalize any non normal values however after running the shapiro test on all data points was disappointed to find that all features seemed to already be normal. I have chosen to use SVC Model as I have found it to have good accuarcy performance, and is cool. After running Grid Search on it it appears that the only change from default that I found that improves accuracy is changing the kernel to linear instead of rbf. Finally I trained a dummy algorithm for comparison with the actual model in order to give a benchmark and show that the model is doing something useful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
